{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [sample solution that works]\n",
    "\n",
    "# This tutorial will bring you through your first deep reinforcement learning model\n",
    "\n",
    "\n",
    "* Seaquest game as an example\n",
    "* Training a simple lasagne neural network for Q_learning objective\n",
    "\n",
    "\n",
    "## About OpenAI Gym\n",
    "\n",
    "* Its a recently published platform that basicly allows you to train agents in a wide variety of environments with near-identical interface.\n",
    "* This is twice as awesome since now we don't need to write a new wrapper for every game\n",
    "* Go check it out!\n",
    "  * Blog post - https://openai.com/blog/openai-gym-beta/\n",
    "  * Github - https://github.com/openai/gym\n",
    "\n",
    "\n",
    "## New to Lasagne and AgentNet?\n",
    "* We only require surface level knowledge of theano and lasagne, so you can just learn them as you go.\n",
    "* Alternatively, you can find Lasagne tutorials here:\n",
    " * Official mnist example: http://lasagne.readthedocs.io/en/latest/user/tutorial.html\n",
    " * From scratch: https://github.com/ddtm/dl-course/tree/master/Seminar4\n",
    " * From theano: https://github.com/craffel/Lasagne-tutorial/blob/master/examples/tutorial.ipynb\n",
    "* This is pretty much the basic tutorial for AgentNet, so it's okay not to know it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game and check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global params.\n",
    "\n",
    "#game title. full list of games = http://yavar.naddaf.name/ale/list_of_current_games.html\n",
    "GAME=\"Skiing-v0\"\n",
    "\n",
    "#number of parallel agents and batch sequence length (frames)\n",
    "N_AGENTS = 10\n",
    "SEQ_LENGTH = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#game image will be resized from (210,160) to your image_size. \n",
    "#You may want a bigger image for your homework assignment IF you want a larger NN\n",
    "IMAGE_W,IMAGE_H = IMAGE_SIZE =(105,80)\n",
    "from scipy.misc import imresize\n",
    "def preprocess(obs):\n",
    "    obs= imresize(obs,IMAGE_SIZE)\n",
    "    return obs.mean(-1)/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-23 23:27:51,469] Making new env: Skiing-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f30fb0e06d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASAAAAFjCAYAAACdT9ZCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f30fd689e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "#creating a game\n",
    "atari = gym.make(GAME)\n",
    "\n",
    "action_names = np.array(atari.get_action_meanings())\n",
    "\n",
    "obs = atari.step(0)[0]\n",
    "\n",
    "plt.imshow(preprocess(obs),interpolation='none',cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic agent setup\n",
    "Here we define a simple agent that maps game images into Qvalues using simple convolutional neural network.\n",
    "\n",
    "![scheme](https://s18.postimg.org/gbmsq6gmx/dqn_scheme.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce 840M (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "theano.config.floatX = 'float32'\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer\n",
    "\n",
    "#image observation at current tick goes here, shape = (sample_i,x,y)\n",
    "observation_layer = InputLayer((None,IMAGE_W,IMAGE_H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.memory import WindowAugmentation,LSTMCell\n",
    "\n",
    "#store 4-tick window in order to perceive motion of objects\n",
    "prev_window = InputLayer((None,4,IMAGE_W,IMAGE_H))\n",
    "\n",
    "#update rule for this window\n",
    "current_window = WindowAugmentation(observation_layer,prev_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import Conv2DLayer,Pool2DLayer,DenseLayer,batch_norm,dropout\n",
    "\n",
    "#main neural network body\n",
    "#<build network body here>\n",
    "nn = Conv2DLayer(current_window, num_filters=32, filter_size=(3,3))\n",
    "nn = Pool2DLayer(nn, pool_size=2)\n",
    "\n",
    "#please set this to your last layer for convenience\n",
    "last_layer = DenseLayer(nn, num_units=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a layer that predicts Qvalues for all actions.\n",
    "# Just adense layer with corresponding number of units and no nonlinearity (lasagne.nonlinearity.linear)\n",
    "n_actions = atari.action_space.n\n",
    "qvalues_layer = DenseLayer(last_layer,\n",
    "                num_units = n_actions,\n",
    "                nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To pick actions, we use an epsilon-greedy resolver (epsilon is a property)\n",
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "\n",
    "action_layer = EpsilonGreedyResolver(qvalues_layer,name=\"e-greedy action picker\")\n",
    "action_layer.epsilon.set_value(np.float32(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights the regular way\n",
    "weights = lasagne.layers.get_all_params(action_layer,trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, agent\n",
    "We declare that this network is and MDP agent with such and such inputs, states and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "#all together\n",
    "agent = Agent(observation_layers=observation_layer,                    #observations\n",
    "              policy_estimators=(qvalues_layer),                       #whatever else you want to monitor\n",
    "              action_layers=action_layer,                               #actions\n",
    "              agent_states={current_window:prev_window},               #dict of memory states\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-23 23:31:34,521] Making new env: Skiing-v0\n",
      "[2017-01-23 23:31:34,622] Making new env: Skiing-v0\n",
      "[2017-01-23 23:31:34,772] Making new env: Skiing-v0\n",
      "[2017-01-23 23:31:34,916] Making new env: Skiing-v0\n",
      "[2017-01-23 23:31:35,047] Making new env: Skiing-v0\n",
      "[2017-01-23 23:31:35,141] Making new env: Skiing-v0\n",
      "[2017-01-23 23:31:35,233] Making new env: Skiing-v0\n",
      "[2017-01-23 23:31:35,323] Making new env: Skiing-v0\n",
      "[2017-01-23 23:31:35,436] Making new env: Skiing-v0\n",
      "[2017-01-23 23:31:35,544] Making new env: Skiing-v0\n"
     ]
    }
   ],
   "source": [
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "\n",
    "pool = EnvPool(agent,GAME, N_AGENTS,preprocess_observation=preprocess) #see docs on what it's capabale of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['LEFT' 'LEFT' 'LEFT' 'LEFT' 'LEFT' 'LEFT' 'NOOP']\n",
      " ['LEFT' 'LEFT' 'LEFT' 'LEFT' 'LEFT' 'LEFT' 'RIGHT']]\n",
      "[[-7. -3. -5. -7. -6. -4.  0.]\n",
      " [-7. -3. -7. -5. -6. -5.  0.]]\n",
      "CPU times: user 616 ms, sys: 48 ms, total: 664 ms\n",
      "Wall time: 664 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "_,action_log,reward_log,_,_,_  = pool.interact(7)\n",
    "\n",
    "print(action_names[action_log][:2])\n",
    "print(reward_log[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load first sessions (this function calls interact and remembers sessions)\n",
    "pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-23 23:31:46,621] Making new env: Skiing-v0\n",
      "[2017-01-23 23:31:46,732] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-01-23 23:31:46,736] Clearing 4 monitor files from previous run (because force=True was provided)\n",
      "[2017-01-23 23:31:46,795] Starting new video recorder writing to /home/anya/ipython/ml_seminars/records/openaigym.video.1.13502.video000000.mp4\n",
      "[2017-01-23 23:33:51,751] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/anya/ipython/ml_seminars/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 5997 timesteps with reward=-30000.0\n"
     ]
    }
   ],
   "source": [
    "action_layer.epsilon.set_value(0)\n",
    "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"records/openaigym.video.0.13502.video000000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import os\n",
    "from random import choice\n",
    "video_path = choice([os.path.join(\"records\",fname) \n",
    "                     for fname in os.listdir(\"records\") \n",
    "                     if fname.endswith(\".mp4\")])\n",
    "\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "* Get reference Qvalues according to Qlearning algorithm\n",
    "* Train on environment interaction sessions\n",
    " * Such sessions are sequences of observations, agent memory, actions, q-values,etc\n",
    "* Implement Q-learning loss & minimize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Some state variables has different dtype/shape from init .",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-2ac4f4bcbfbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mreplay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msession_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moptimize_experience_replay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/agentnet-0.10.2-py2.7.egg/agentnet/agent/mdp_agent.pyc\u001b[0m in \u001b[0;36mget_sessions\u001b[0;34m(self, environment, session_length, batch_size, initial_env_states, initial_observations, initial_hidden, experience_replay, unroll_scan, return_automatic_updates, optimize_experience_replay, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# convert sequence layers into actual theano variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mtheano_expressions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_layers_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mn_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_layers_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anya/.local/lib/python2.7/site-packages/lasagne/layers/helper.pyc\u001b[0m in \u001b[0;36mget_output\u001b[0;34m(layer_or_layers, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m                                  \u001b[0;34m\"mapping this layer to an input expression.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                                  % layer)\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mall_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_for\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/agentnet-0.10.2-py2.7.egg/agentnet/agent/recurrence.pyc\u001b[0m in \u001b[0;36mget_output_for\u001b[0;34m(self, inputs, recurrence_flags, **kwargs)\u001b[0m\n\u001b[1;32m    581\u001b[0m                                   \u001b[0moutputs_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                                   \u001b[0mnon_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnonsequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                                   \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m                                   )\n\u001b[1;32m    585\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anya/.local/lib/python2.7/site-packages/lasagne/utils.pyc\u001b[0m in \u001b[0;36munroll_scan\u001b[0;34m(fn, sequences, outputs_info, non_sequences, n_steps, go_backwards)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mstep_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprev_vals\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnon_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0mout_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m             \u001b[0;31m# The returned values from step can be either a TensorVariable,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;31m# a list, or a tuple.  Below, we force it to always be a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/agentnet-0.10.2-py2.7.egg/agentnet/agent/recurrence.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    529\u001b[0m             new_states = [prev_state.type.convert_variable(state)\n\u001b[1;32m    530\u001b[0m                             for (prev_state,state) in zip(prev_states,new_states)]\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Some state variables has different dtype/shape from init .\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m             new_outputs = [prev_out.type.convert_variable(out)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Some state variables has different dtype/shape from init ."
     ]
    }
   ],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay\n",
    "\n",
    "_,_,_,_,qvalues_seq, = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    optimize_experience_replay=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qvalues_seq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-f3a697a5f9e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#qvalues at current tick.  shape = [batch_i,time_tick, action_id]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mqvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqvalues_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#q-values at next tick. shape = [batch_i,time_tick, action_id], padded with zeros for math simplicity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qvalues_seq' is not defined"
     ]
    }
   ],
   "source": [
    "import theano.tensor as T\n",
    "\n",
    "#actions, shape= [batch_i, time_tick]\n",
    "actions = replay.actions[0]\n",
    "\n",
    "#rewards [batch_i,time_tick]\n",
    "rewards = replay.rewards\n",
    "\n",
    "#session indicator (0 means session ended) [batch_i,time_tick]\n",
    "is_alive = replay.is_alive\n",
    "\n",
    "#qvalues at current tick.  shape = [batch_i,time_tick, action_id]\n",
    "qvalues = qvalues_seq\n",
    "\n",
    "#q-values at next tick. shape = [batch_i,time_tick, action_id], padded with zeros for math simplicity\n",
    "next_qvalues = T.concatenate([qvalues[:, 1:],\n",
    "                              T.zeros_like(qvalues[:,:1,:]),],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-64-6989593db05b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-64-6989593db05b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    rewards = <clip rewards to [-1,1] range. Alternatively, scale them or just mind your learning rates>\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#clip rewards to [-1,+1] to avoid explosion.\n",
    "rewards = <clip rewards to [-1,1] range. Alternatively, scale them or just mind your learning rates>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#compute q-values for best actions\n",
    "optimal_next_qvalues = \n",
    "#<qvalue for optimal action. Aggregate over next_qvalues>\n",
    "\n",
    "gamma=0.99\n",
    "\n",
    "# target Qvalues, r + gamma*max_a' Q(s', a')\n",
    "reference_qvalues = <reference qvalues, r+gamma*Q(s_next,a_max)>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.learning.helpers import get_end_indicator\n",
    "\n",
    "#indicator of session end [batch_i,time_tick]\n",
    "is_end = get_end_indicator(is_alive)\n",
    "\n",
    "#set reference qvalues at session end to just the immediate rewards\n",
    "reference_qvalues = T.switch(is_end,rewards,reference_qvalues)\n",
    "\n",
    "#consider constant\n",
    "reference_qvalues = theano.gradient.disconnected_grad(reference_qvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.learning.helpers import get_action_Qvalues\n",
    "\n",
    "#q-values for chosen actions [batch_i,time_tick]\n",
    "predicted_qvalues = get_action_Qvalues(qvalues,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss for Qlearning = (Q(s,a) - (r+gamma*Q(s',a_max)))^2 at each tick\n",
    "elwise_mse_loss = <elementwise loss for Q-learning>\n",
    "\n",
    "#exclude last tick (zeros)\n",
    "elwise_mse_loss = T.set_subtensor(elwise_mse_loss[:,-1],0)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "loss = (elwise_mse_loss*is_alive).sum() / is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute weight updates. Replace with any optimizer you want\n",
    "updates = <optimize loss over nn weights using your favorite algorithm>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile train function\n",
    "import theano\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "\n",
    "#full game rewards\n",
    "rewards = {epoch_counter:untrained_reward}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_epsilon(epoch_counter):\n",
    "    \"\"\"\n",
    "    a function which outputs current epsilon for e-greedy exploration given training iteration.\n",
    "    \"\"\"\n",
    "    <implement me!>\n",
    "    return 0.1\n",
    "\n",
    "#a visualizer\n",
    "plt.plot(np.linspace(0,50000),[get_epsilon(i) for i in np.linspace(0,50000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#the loop may take eons to finish.\n",
    "#consider interrupting early.\n",
    "for i in xrange(10**7):    \n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    current_epsilon = get_epsilon(epoch_counter)\n",
    "    action_layer.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    #train\n",
    "    pool.update(SEQ_LENGTH)\n",
    "    loss = train_step()\n",
    "    \n",
    "    if epoch_counter%10==0:\n",
    "        #average reward per game tick in current experience replay pool\n",
    "        print(\"iter=%i\\tepsilon=%.3f\\tloss=%.3f\"%(epoch_counter,current_epsilon,loss))\n",
    "        \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%500 ==0:\n",
    "        n_games = 10\n",
    "        action_layer.epsilon.set_value(0)\n",
    "        rewards[epoch_counter] = pool.evaluate( record_video=False,n_games=n_games,verbose=False)\n",
    "        print(\"Current score(mean over %i) = %.3f\"%(n_games,np.mean(rewards[epoch_counter])))\n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results\n",
    " * Here we plot learning curves and sample testimonials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "t,r = zip(*sorted(rewards.items(),key=lambda k:k[0]))\n",
    "plt.plot(t,pd.ewma(np.concatenate(r),alpha=0.1))\n",
    "plt.title(\"moving average of rewards over ticks of training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "action_layer.epsilon.set_value(0.05)\n",
    "rw = pool.evaluate(n_games=20,save_path=\"./records\",record_video=False)\n",
    "print(\"mean session score=%f.5\"%np.mean(rw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "#select the one you want\n",
    "video_path=\"./records/openaigym.video.0.13.video000000.mp4\"\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.utils.persistence import save,load\n",
    "save(action_layer,\"gopher.pcl\")\n",
    "#load(action_layer,\"gopher.pcl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Once you got it working,\n",
    "Try building a network that maximizes the final score\n",
    "\n",
    "* Moar lasagne stuff: convolutional layers, batch normalization, nonlinearities and so on\n",
    "* Recurrent agent memory layers, GRUMemoryLayer, etc\n",
    "* Different reinforcement learning algorithm (p.e. qlearning_n_step), other parameters\n",
    "* Experience replay pool\n",
    "\n",
    "\n",
    "Look for info?\n",
    "* [lasagne doc](http://lasagne.readthedocs.io/en/latest/)\n",
    "* [agentnet doc](http://agentnet.readthedocs.io/en/latest/)\n",
    "* [gym homepage](http://gym.openai.com/)\n",
    "\n",
    "\n",
    "You can also try to expand to a different game: \n",
    " * all OpenAI Atari games are already compatible, you only need to change GAME_TITLE\n",
    " * Other discrete action space environments are also accessible this way\n",
    " * For continuous action spaces, either discretize actions or use continuous RL algorithms (e.g. .learning.dpg_n_step)\n",
    " * Adapting to a custom non-OpenAI environment can be done with a simple wrapper\n",
    " \n",
    " \n",
    "__Good luck!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
